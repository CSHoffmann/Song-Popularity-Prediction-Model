---
title: "Parker"
author: "Parker Brotman"
date: "5/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
```

```{r}
top50 = read_csv('../data/top50cleaned.csv',col_types = cols(Genre = col_factor()))
glimpse(top50)
attach(top50)
```


# Predicting Genre with SVM
```{r}
library(e1071)
```

## Model Selection
First, let's determine which kernel and costs to use.
```{r}
set.seed(1)
S1tuned = tune(svm,Genre ~ . -Genre - X1 - Track.Name - Artist.Name,data = top50,kernel='linear',ranges = list(cost=10^seq(-3,3)) )
```
```{r}
set.seed(1)
S2tuned = tune(svm,Genre ~ . -Genre - X1 - Track.Name - Artist.Name,data = top50,kernel='polynomial',ranges = list(cost=10^seq(-3,3)) )
```
```{r}
set.seed(1)
S3tuned = tune(svm,Genre ~ . -Genre - X1 - Track.Name - Artist.Name,data = top50,kernel='radial',ranges = list(cost=10^seq(-3,3)) )
```
```{r}
set.seed(1)
S4tuned = tune(svm,Genre ~ . -Genre - X1 - Track.Name - Artist.Name,data = top50,kernel='sigmoid',ranges = list(cost=10^seq(-3,3)) )
```

```{r}
summary(S1tuned)
```
```{r}
summary(S2tuned)
```
```{r}
summary(S3tuned)
```
```{r}
summary(S4tuned)
```

The two most promising models appear to be the Linear kernel with cost = .1 and the Radial kernel with cost = 1. Now, let's evaluate the predictive accuracy of these two models.


## Evaluating Predictive Accuracy
This is a small dataset, so rather than splitting the data into training/testing, let's use LOOCV to determine predictive accuracy.
```{r}
n = nrow(top50)
preds = c()
for (i in 1:n) {
  d.train = top50[-i,]
  d.test = top50[i,]
  s.temp = svm(Genre ~ . -Genre - X1 - Track.Name - Artist.Name,data = d.train,kernel='linear',cost=.1)
  yhat = predict(s.temp,d.test['Genre'])[i]
  preds = append(preds,yhat)
}
preds = recode_factor(preds,`1` ='Pop', `2`='Latin',`3`='Hip Hop')
table(Predicted= preds,Actual= Genre)
svm.linear.MSPE = mean(preds == Genre)
svm.linear.MSPE
```
```{r}
n = nrow(top50)
preds = c()
for (i in 1:n) {
  d.train = top50[-i,]
  d.test = top50[i,]
  s.temp = svm(Genre ~ . -Genre - X1 - Track.Name - Artist.Name,data = d.train,kernel='radial',cost=1)
  yhat = predict(s.temp,d.test['Genre'])[i]
  preds = append(preds,yhat)
}
preds = recode_factor(preds,`1` ='Pop', `2`='Latin',`3`='Hip Hop')
table(Predicted= preds,Actual= Genre)
mean(preds == Genre)
svm.radial.MSPE = mean(preds == Genre)
svm.radial.MSPE
```

The Linear kernal model has a classification rate of .7, while the Radial kernal model has a classification rate of .72. The first model is better at classifying Latin, but can predict Hip Hop with only 50% accuracy. Meanwhile, second model is better at classifying Pop and Hip Hop, but misclassifies Latin as Pop over 2/3 of the time.


## Visualizing the Models
SVM is not a very interpretable model, and it is tough to visualize with more than two dimensions. However, let's refit SVM models using just Loudness and Popularity, and then visualize them.
```{r}
plot(svm(Genre ~ Loudness..dB.. + Popularity,data = top50,kernel='linear',cost=.1),top50,Loudness..dB.. ~ Popularity)
```
```{r}
plot(svm(Genre ~ Loudness..dB.. + Popularity,data = top50,kernel='radial',cost=1),top50,Loudness..dB.. ~ Popularity)
```

In both models, most songs are classified as Pop, except louder and more popular songs are classified as Latin.



# LDA/QDA
```{r}
library(MASS)
```

## Evaluating Predictive Accuracy
We will evaluate the MSPE of the models using LOOCV, which is conveniently built into the `lda()` and `qda()` functions as an option.
```{r}
lda.cv = lda(Genre ~ . -Genre - X1 - Track.Name - Artist.Name,data = top50,CV = TRUE)
table(Predicted= lda.cv$class,Actual= Genre)
lda.MSPE = mean(Genre == lda.cv$class)
lda.MSPE
```


```{r}
qda.cv = qda(Genre ~ . -Genre - X1 - Track.Name - Artist.Name,data = top50,CV = TRUE)
table(Predicted= qda.cv$class,Actual= Genre)
qda.MSPE = mean(Genre == qda.cv$class)
qda.MSPE
```

The LDA performs which better than the QDA, which indicates that a linear decision boundary, thus equal variances between prior probabilities. The LDA has a classification rate of .78. The QDA, on the other hand, has a dismal classification rate of .52, which is only barely better than guessing.

## Plotting LDA
```{r}
lda.fit = lda(Genre ~ . -Genre - X1 - Track.Name - Artist.Name,data = top50)
plot(lda.fit)
```

We can see that LDA does a nice job separating the three genres, but that there is some overlap in the middle.


# Comparison

Let us compare the MSPE for each of our four models that predict Genre:
```{r}
c(SVM.Lin= svm.linear.MSPE, SVM.Rad= svm.radial.MSPE, LDA= lda.MSPE, QDA= qda.MSPE)
```
LDA has the best MSPE, while the two SVM models are not too far behind. QDA, meanwhile has a poor MSPE. It is also worth noting a finding which is not captured by MSPE: LDA is much better at correctly classifying Latin and Hip Hop, whereas SVM takes advantage of the class imbalance by overclassifying to Pop.
